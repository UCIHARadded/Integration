# ğŸ”¬ Integration Framework for Domain Generalization with GNNs, SHAP, and Curriculum Learning

## ğŸ§  Overview

This repository provides an integrated framework for research in Domain Generalization (DG) using Graph Neural Networks (GNNs), SHAP-based interpretability, and Curriculum Learning strategies. It extends the DIVERSIFY architecture with automation of latent domain estimation, curriculum learning and SHAP evaluation tools. The framework supports continual learning setups, clustering-based domain estimation, and perturbation-based interpretability evaluation.

## ğŸš€ Core Pipelines

### ğŸ”§ Training Pipeline

The training phase includes data loading, clustering-based domain estimation, curriculum sorting, model training with an optional CNN backbone, and domain-specific optimization.

        train.py --> datautil/ --> alg/ --> loss/ --> network/

Training Pipeline Illustration:

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Raw Data  â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Domain Split â”‚<â”€â”€â”€â–¶â”‚ Cluster Estimation â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼                         â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Curriculum   â”‚â”€â”€â”€â”€â–¶ â”‚ Model Training   â”‚
      â”‚ Learning     â”‚      â”‚      (CNN)       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Save Trained     â”‚
                            â”‚ Weights & Logs   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š Evaluation Pipeline

Evaluation supports test-time inference, SHAP-based explainability metrics (flip rate, coherence, AOPC), and generalization gap measurements across domains.

Evaluation Pipeline Illustration:

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Test Data  â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Load Trained Model â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Inference & SHAP Analysis  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Explanation Metricsâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ¨ Key Features

    âœ… Automated Latent Domain Estimation using clustering.

    âœ… Curriculum Learning for sequential and staged domain training.

    âœ… SHAP-based Evaluations: Flip Rate, AOPC, Coherence, Sparsity.

    âœ… GNN Integration: Uses graph-based backbones like GCN.

    âœ… Cross-domain Testing with configurable datasets.

    âœ… Extensible Pipeline for novel DG/DA experiments.

ğŸ“ File Structure

Integration-main/
â”œâ”€â”€ alg/                        # Core domain generalization algorithms
â”‚   â”œâ”€â”€ algs/                   # Specific algorithm implementations (e.g. diversify.py)
â”‚   â”œâ”€â”€ alg.py                  # Main algorithm controller
â”‚   â””â”€â”€ modelopera.py           # Model operations
â”œâ”€â”€ datautil/                   # Data loaders, curriculum sorting, clustering
â”‚   â”œâ”€â”€ actdata/                # Activity recognition dataset utilities
â”‚   â”œâ”€â”€ getcurriculumloader.py
â”‚   â”œâ”€â”€ getdataloader_single.py
â”‚   â””â”€â”€ cluster.py              # Clustering logic for domain estimation
â”œâ”€â”€ loss/                       # Custom loss functions
â”‚   â””â”€â”€ common_loss.py
â”œâ”€â”€ network/                    # Model architectures
â”‚   â”œâ”€â”€ act_network.py          # CNN for activity data
â”‚   â”œâ”€â”€ Adver_network.py        # Adversarial components
â”‚   â””â”€â”€ common_network.py       # Base models
â”œâ”€â”€ utils/                      # Utility functions and argument parsing
â”‚   â”œâ”€â”€ params.py
â”‚   â””â”€â”€ util.py
â”œâ”€â”€ shap_utils.py               # SHAP evaluation metrics
â”œâ”€â”€ train.py                    # Entry point for training
â”œâ”€â”€ env.yml                     # Conda environment spec
â””â”€â”€ README.md                   # This file

ğŸ“Š Supported Datasets

Currently supports activity recognition datasets, especially cross-subject and cross-location setups via:

    actdata/cross_people.py

    UCI HAR and WESAD datasets (with preprocessing expected in CSV format)

â–¶ï¸ How to Run
ğŸ“¦ Setup

conda env create -f env.yml
conda activate diversify_env

ğŸ‹ï¸â€â™‚ï¸ Training

python train.py \
  --data_dir ./data/ \
  --task act \
  --algorithm diversify \
  --latent_domain_num 3 \
  --max_epoch 20 \
  --local_epoch 2 \
  --enable_shap True

ğŸ§ª Evaluation

After training, use the same script with --eval flag (if implemented) or embed evaluation in train.py by enabling SHAP.
ğŸ“‚ Outputs and Artifacts

    ./checkpoints/: Trained model weights.

    ./logs/: Training loss and accuracy logs.

    ./results/: SHAP metrics, AOPC plots, domain generalization reports.

Typical evaluation output includes:
Metric	Description
Flip Rate	Change in prediction after key feature masking
AOPC	Confidence drop curve area
Coherence	Agreement across similar inputs
Sparsity	Minimum features required to explain
ğŸ” In-depth Analysis

    Latent Domain Clustering: Uses KMeans-based latent splitting via datautil/cluster.py.

    Curriculum Training: Orders samples by feature norm or confidence before training.

    SHAP Explanations: shap_utils.py implements post-hoc perturbation evaluations and ranking metrics.

    GNN Support: Drop-in support via network/common_network.py using PyTorch Geometric-style layers.

    Cross-Domain Testing: Evaluates generalization across unseen person IDs or locations.

ğŸ“œ License

This project is licensed under the MIT License.

MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy...
